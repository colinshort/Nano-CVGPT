# Nano-CVGPT
## An attempt to take a tiny GPT-2-like LLM and rewrite it to use complex-valued numbers instead of real numbers

The following are the papers I pulled ideas and code from for this learning exercise:
- "Theory And Implementation Of Complex-Valued Neural Networks"  https://arxiv.org/pdf/2302.08286
- "Encoding Word Order In Complex Embeddings"   https://arxiv.org/pdf/1912.12333
  - Note: I'm still working on incorporating this new embedding strategy
- "Language Models are Few-Shot Learners"  https://arxiv.org/pdf/2005.14165
- "Building Blocks For A Complex-Valued Transformer Architecture"  https://arxiv.org/pdf/2306.09827
- "Complex Transformer: A Framework For Modeling Complex-Valued Sequence"  https://arxiv.org/pdf/1910.10202

I sourced the training data and derived the real-valued baseline code from Andrej Karpathy's awesome YouTube video: https://www.youtube.com/watch?v=kCc8FmEb1nY
